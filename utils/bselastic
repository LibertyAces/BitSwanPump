#!/usr/bin/env python3
import argparse
import os
import json
import pprint
import random
import re
import datetime
import time
import requests

def parse_cmdline():
	# Parse args
	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawDescriptionHelpFormatter,
		description='''Manage ElasticSearch.''')

	subparsers = parser.add_subparsers(help='commands')

	# An load_index_template command
	load_template_parser = subparsers.add_parser('load_index_template', help='loads index templates into ElasticSearch')
	load_template_parser.add_argument('DIR', action='store', help='a directory to seek for template')
	load_template_parser.add_argument('URL', action='store', help='a ElasticSearch URL to write to')
	load_template_parser.set_defaults(COMMAND='load_index_template')

	# An cleanup command
	cleanup_parser = subparsers.add_parser('cleanup', help='Closes old and deletes empty indexes')
	cleanup_parser.add_argument('URL', action='store', help='a ElasticSearch URL')
	cleanup_parser.add_argument('--min-date',
		action='store',
		help='Min date (format yyyy-mm-dd)',
		default='1970-01-01')
	cleanup_parser.add_argument('--max-date',
		action='store',
		help='Max date (format yyyy-mm-dd)',
		default='1970-01-01')
	cleanup_parser.add_argument('--exclude',
		action='store',
		help='Indicies with matching prefix will be excluded',
		default=None)
	cleanup_parser.set_defaults(COMMAND='cleanup')

	# An reopen command
	reopen_parser = subparsers.add_parser('reopen', help='reopen closed indexes in time range')
	reopen_parser.add_argument('URL', action='store', help='a ElasticSearch URL')
	reopen_parser.add_argument('--min-date',
		action='store',
		help='Min date (format yyyy-mm-dd)',
		default='1970-01-01')
	reopen_parser.add_argument('--max-date',
		action='store',
		help='Max date (format yyyy-mm-dd)',
		default='1970-01-01')
	reopen_parser.add_argument('--exclude',
		action='store',
		help='Indicies with matching prefix will be excluded',
		default=None)
	reopen_parser.set_defaults(COMMAND='reopen')

	# Close command
	close_parser = subparsers.add_parser('close', help='close open indexes in time range')
	close_parser.add_argument('URL', action='store', help='a ElasticSearch URL')
	close_parser.add_argument('--min-date',
		action='store',
		help='Min date (format yyyy-mm-dd)',
		default='1970-01-01')
	close_parser.add_argument('--max-date',
		action='store',
		help='Max date (format yyyy-mm-dd)',
		default='1970-01-01')
	close_parser.add_argument('--exclude',
		action='store',
		help='Indicies with matching prefix will be excluded',
		default=None)
	close_parser.set_defaults(COMMAND='close')

	# Delete command
	delete_parser = subparsers.add_parser('delete', help='delete indexes in time range')
	delete_parser.add_argument('URL', action='store', help='a ElasticSearch URL')
	delete_parser.add_argument('--min-date',
		action='store',
		help='Min date (format yyyy-mm-dd)',
		default='1970-01-01')
	delete_parser.add_argument('--max-date',
		action='store',
		help='Max date (format yyyy-mm-dd)',
		default='1970-01-01')
	delete_parser.add_argument('--exclude',
		action='store',
		help='Indicies with matching prefix will be excluded',
		default=None)
	delete_parser.set_defaults(COMMAND='delete')

	# Force merge command
	forcemerge_parser = subparsers.add_parser('force-merge', help='forces merge and shrinks all indices')
	forcemerge_parser.add_argument('URL', action='store', help='an ElasticSearch URL')
	forcemerge_parser.add_argument('--max-cpu-load',
		action='store',
		help='CPU usage threshold that will trigger the merge',
		default='1')
	forcemerge_parser.add_argument('--backup-node',
		action='store',
		help='Node where the shrunk indices will be stored (defaults to data node with the lowest memory usage)',
		default=None)
	forcemerge_parser.add_argument('--exclude',
		action='store',
		help='Indices with matching prefix will be excluded',
		default=None)
	forcemerge_parser.set_defaults(COMMAND='force-merge')

	# Shrink command
	forcemerge_parser = subparsers.add_parser('shrink', help='shrink a specified index')
	forcemerge_parser.add_argument('URL', action='store', help='an ElasticSearch URL')
	forcemerge_parser.add_argument('--index',
		action='store',
		help='Index to shrink')
	forcemerge_parser.add_argument('--backup-node',
		action='store',
		help='Node where the shrunk index will be stored (by default, a warm node is picked at random)',
		default=None)
	forcemerge_parser.add_argument('--shrunk-prefix',
		action='store',
		help='Prefix of the resulting shrunk index',
		default='shrunk_')
	forcemerge_parser.set_defaults(COMMAND='shrink')

	return parser.parse_args()


def req_close_index(es_url, index_name):
	url_close = es_url + '{}/_close'.format(index_name)
	r = requests.post(url_close, json={})
	return r.json()


def req_delete_index(es_url, index_name):
	url_close = es_url + '{}'.format(index_name)
	r = requests.delete(url_close, json={})
	return r.json()


def req_maxmin_timestamp(es_url, index_name):
	maxmin_request = {
		"aggs" : {
			"max_timestamp" : { "max" : { "field" : "@timestamp" } },
			"min_timestamp" : { "min" : { "field" : "@timestamp" } }
		}
	}
	url_maxmin = es_url + '{}/_search?size=0'.format(index_name)
	r = requests.post(url_maxmin, json=maxmin_request)
	return r.json()


def min_datetime_from_maxmin(maxmin_obj):
	v = maxmin_obj['aggregations']['min_timestamp']['value']
	if v is not None:
		min_date = datetime.datetime.utcfromtimestamp(v/1000.0)
	else:
		min_date = None
	return min_date


def max_datetime_from_maxmin(maxmin_obj):
	v = maxmin_obj['aggregations']['max_timestamp']['value']
	if v is not None:
		max_date = datetime.datetime.utcfromtimestamp(v/1000.0)
	else:
		max_date = None
	return max_date


def datetime_ranges_collide(a_min_datetime, a_max_datetime, b_min_datetime, b_max_datetime):
	assert a_min_datetime <= a_max_datetime
	assert b_min_datetime <= b_max_datetime

	if b_min_datetime < a_min_datetime and b_max_datetime > a_min_datetime:
		# A   |---| ||   |--|
		# B |---|   || |------|
		#===============
		return True
	elif b_min_datetime > a_min_datetime and b_min_datetime < a_max_datetime:
		# A |---|   || |-------|
		# B   |---| ||   |---|
		return True
	else:
		return False


def datetime_range_a_is_within_b(a_min_datetime, a_max_datetime, b_min_datetime, b_max_datetime):
	assert a_min_datetime <= a_max_datetime
	assert b_min_datetime <= b_max_datetime

	# A |------|
	# B |--------|
	return a_min_datetime >= b_min_datetime and a_max_datetime < b_max_datetime


def COMMAND_load_index_template(DIR, URL):

	# Compile list of templates
	template_files = []
	for root, subdirs, files in os.walk(DIR):
		if 'es_index_template.json' in files:
			template_files.append(os.path.join(root, 'es_index_template.json'))

	for tf in template_files:
		print("Loading {}".format(tf))
		obj = None
		try:
			b = open(tf,'r').read()
			# Strip comments
			b = re.sub(r"//.*$", "", b, flags=re.M)

			obj = json.loads(b)
		except Exception as e:
			print("Failed to load {}: {}".format(tf, e))
			continue

		deploy_to = obj.pop('!deploy_to')


		url = URL
		if url[-1:] != '/': url += '/'

		url += '_template/'+deploy_to
		print(" to {}".format(url))

		r = requests.put(url, json=obj)
		print(r.text)


def COMMAND_cleanup(URL, args_min_date, args_max_date, exclude_prefix):
	url = URL
	if url[-1:] != '/': url += '/'

	args_min_datetime = datetime.datetime.strptime(args_min_date, "%Y-%m-%d")
	args_max_datetime = datetime.datetime.strptime(args_max_date, "%Y-%m-%d")
	args_max_datetime = args_max_datetime + datetime.timedelta(days=1)
	assert args_min_datetime < args_max_datetime

	print("Communicate with: {}".format(url))

	url_indices = url + '_cat/indices?format=json'
	r = requests.get(url_indices)
	indices = r.json()
	#json.dump(indices, open('dump.json', 'wt'))
	#indices = json.load(open('dump.json', 'rt'))

	empty_indices = []
	old_count = 0
	not_green_count = 0
	closed_count = 0

	indices = sorted(indices, key=lambda index:index['index'].rsplit('_')[-1], reverse=False)

	for index in indices:
		try:
			name = index['index']

			if name.startswith('.'):
				print("Index {} will be excluded. Indexes that start with '.' are always excluded".format(name))
				continue

			if exclude_prefix is not None and name.startswith(exclude_prefix):
				print("Index {0} will be excluded because it matches prefix {1}".format(name, exclude_prefix))
				continue

			if index['health'] != 'green':
				#print("Index {index} health is {health} - skipping".format(**index))
				not_green_count += 1
				continue

			if index['status'] != 'open':
				continue

			if int(index['docs.count']) == 0:
				# Delete empty indicies
				print("Deleting '{}' (doc.count: {})".format(name, index['docs.count']))
				url_delete_index = url + '{}'.format(name)
				r = requests.delete(url_delete_index)
				result = r.json()
				if result.get('acknowledged') != True:
					print("Failed to delete index '{}': {}".format(name, result))
				continue

			maxmin_obj = req_maxmin_timestamp(url, name)
			max_datetime = max_datetime_from_maxmin(maxmin_obj)
			min_datetime = min_datetime_from_maxmin(maxmin_obj)


			if  min_datetime is not None \
				and max_datetime is not None \
				and datetime_range_a_is_within_b(
					min_datetime, max_datetime,
					args_min_datetime, args_max_datetime
				):
				old_count += 1
				# Delete close old indicies

				print("Closing index: {}\n\tMax: {}\n\tMin: {}\n\tCount: {}\n".format(
					name,
					max_datetime, min_datetime,
					index['docs.count']
				))

				url_close = url + '{}/_close'.format(name)
				r = requests.post(url_close, json={})
				result = r.json()

				if result.get('acknowledged') != True:
					print("Failed to close index '{}': {}".format(name, result))

				continue

		
		except KeyboardInterrupt:
			return

		except:
			print("Error in this index:")
			pprint.pprint(index)

	print("Count of total indicies: {}".format(len(indices)))
	print("Count of old indicies: {}".format(old_count))
	print("Count of empty indicies: {}".format(len(empty_indices)))
	print("Count of non-green indicies: {}".format(not_green_count))


def COMMAND_reopen(URL, args_min_date, args_max_date, exclude_prefix, max_server_load):
	url = URL
	if url[-1:] != '/': url += '/'

	args_min_datetime = datetime.datetime.strptime(args_min_date, "%Y-%m-%d")
	args_max_datetime = datetime.datetime.strptime(args_max_date, "%Y-%m-%d")
	args_max_datetime = args_max_datetime + datetime.timedelta(days=1)
	assert args_min_datetime < args_max_datetime

	print("Communicate with: {}".format(url))

	url_indices = url + '_cat/indices?format=json'
	r = requests.get(url_indices)
	indices = r.json()
	# json.dump(indices, open('dump.json', 'wt'))
	# indices = json.load(open('dump.json', 'rt'))

	closed_count = 0
	opened_count = 0

	indices = sorted(indices, key=lambda index:index['index'].rsplit('_')[-1], reverse=False)

	for index in indices:
		try:
			name = index['index']

			if name.startswith('.'):
				print("Index {} will be excluded. Indexes that start with '.' are always excluded".format(name))
				continue

			if exclude_prefix is not None and name.startswith(exclude_prefix):
				print("Index {0} will be excluded because it matches prefix {1}".format(name, exclude_prefix))
				continue

			if index['status'] != 'close':
				continue

			closed_count+=1

			# Open
			print("Opening index {}".format(name))
			url_open = url + '{}/_open'.format(name)
			r = requests.post(url_open, json={})
			result = r.json()


			# Wait until open
			url_stats = url + '_cat/indices/{}?format=json'.format(name)
			is_open = False
			while not is_open:
				r = requests.get(url_stats)
				result = r.json()
				assert len(result) == 1
				is_open = result[0]['status'] == 'open'
				print("Index '{}' {} open.".format(name, "is" if is_open else "is not"))
				if not is_open:
					time.sleep(1)


			# Get max and min date
			res_success = False
			retry_count = 0
			while not res_success:
				maxmin_obj = req_maxmin_timestamp(url, name)
				res_success = "aggregations" in maxmin_obj
				if not res_success:
					retry_count+=1
					if retry_count > 5:
						raise RuntimeError("Max retries exceeded while requesting maxmin timestamp on index '{}'".format(name))
					time.sleep(1)

			max_datetime = max_datetime_from_maxmin(maxmin_obj)
			min_datetime = min_datetime_from_maxmin(maxmin_obj)


			# Close if found time range doesn't collide with selected time range
			if  min_datetime is None \
				or max_datetime is None \
				or not datetime_range_a_is_within_b(
					min_datetime, max_datetime,
					args_min_datetime, args_max_datetime
				):
				print("Closing index {}".format(name))
				result = req_close_index(url, name)
			else:
				opened_count+=1

		except KeyboardInterrupt:
			return

		except Exception as e:
			print("Error in this index:")
			pprint.pprint(index)

	print("Count of total indicies: {}".format(len(indices)))
	print("Count of closed indicies: {}".format(closed_count))
	print("Count of indices that were reopened: {}".format(opened_count))


def COMMAND_close(URL, args_min_date, args_max_date, exclude_prefix):
	url = URL
	if url[-1:] != '/': url += '/'

	args_min_datetime = datetime.datetime.strptime(args_min_date, "%Y-%m-%d")
	args_max_datetime = datetime.datetime.strptime(args_max_date, "%Y-%m-%d")
	args_max_datetime = args_max_datetime + datetime.timedelta(days=1)
	assert args_min_datetime < args_max_datetime

	print("Communicate with: {}".format(url))

	url_indices = url + '_cat/indices?format=json'
	r = requests.get(url_indices)
	indices = r.json()
	# json.dump(indices, open('dump.json', 'wt'))
	# indices = json.load(open('dump.json', 'rt'))

	open_count = 0
	not_green_count = 0
	closed_count = 0

	indices = sorted(indices, key=lambda index:index['index'].rsplit('_')[-1], reverse=False)

	for index in indices:
		try:
			name = index['index']

			if name.startswith('.'):
				print("Index {} will be excluded. Indexes that start with '.' are always excluded".format(name))
				continue

			if exclude_prefix is not None and name.startswith(exclude_prefix):
				print("Index {0} will be excluded because it matches prefix {1}".format(name, exclude_prefix))
				continue

			if index['health'] != 'green' and index['health'] != 'yellow':
				#print("Index {index} health is {health} - skipping".format(**index))
				not_green_count += 1
				continue

			if index['status'] != 'open':
				continue


			open_count+=1

			# Get max and min date
			res_success = False
			retry_count = 0
			while not res_success:
				maxmin_obj = req_maxmin_timestamp(url, name)
				res_success = "aggregations" in maxmin_obj
				if not res_success:
					retry_count+=1
					if retry_count > 5:
						raise RuntimeError("Max retries exceeded while requesting maxmin timestamp on index '{}'".format(name))
					time.sleep(1)

			max_datetime = max_datetime_from_maxmin(maxmin_obj)
			min_datetime = min_datetime_from_maxmin(maxmin_obj)


			# Close if found time range doesn't collide with selected time range
			if  min_datetime is not None \
				and max_datetime is not None \
				and datetime_range_a_is_within_b(
					min_datetime, max_datetime,
					args_min_datetime, args_max_datetime
				):
				print("Closing index {}".format(name))
				result = req_close_index(url, name)
				closed_count+=1
			else:
				print("Index {} will remain open.".format(name))

		except KeyboardInterrupt:
			return

		except Exception as e:
			print("Error in this index:")
			pprint.pprint(index)

	print("Count of total indicies: {}".format(len(indices)))
	print("Count of open indicies: {}".format(open_count))
	print("Count of indices that were closed: {}".format(closed_count))


def COMMAND_delete(URL, args_min_date, args_max_date, exclude_prefix):
	url = URL
	if url[-1:] != '/': url += '/'

	args_min_datetime = datetime.datetime.strptime(args_min_date, "%Y-%m-%d")
	args_max_datetime = datetime.datetime.strptime(args_max_date, "%Y-%m-%d")
	args_max_datetime = args_max_datetime + datetime.timedelta(days=1)
	assert args_min_datetime < args_max_datetime

	print("Communicate with: {}".format(url))

	url_indices = url + '_cat/indices?format=json'
	r = requests.get(url_indices)
	indices = r.json()
	# json.dump(indices, open('dump.json', 'wt'))
	# indices = json.load(open('dump.json', 'rt'))

	delete_count = 0

	indices = sorted(indices, key=lambda index:index['index'].rsplit('_')[-1], reverse=False)

	for index in indices:
		try:
			name = index['index']
			was_closed = False

			if name.startswith('.'):
				print("Index {} will be excluded. Indexes that start with '.' are always excluded".format(name))
				continue

			if exclude_prefix is not None and name.startswith(exclude_prefix):
				print("Index {0} will be excluded because it matches prefix {1}".format(name, exclude_prefix))
				continue

			if index['status'] == 'close':
				was_closed = True

				# Open
				print("Opening index {} to analyze max and min dates".format(name))
				url_open = url + '{}/_open'.format(name)
				r = requests.post(url_open, json={})
				result = r.json()


				# Wait until open
				url_stats = url + '_cat/indices/{}?format=json'.format(name)
				is_open = False
				while not is_open:
					r = requests.get(url_stats)
					result = r.json()
					assert len(result) == 1
					is_open = result[0]['status'] == 'open'
					print("Index '{}' {} open.".format(name, "is" if is_open else "is not"))
					if not is_open:
						time.sleep(1)

			# Get max and min date
			res_success = False
			retry_count = 0
			while not res_success:
				maxmin_obj = req_maxmin_timestamp(url, name)
				res_success = "aggregations" in maxmin_obj
				if not res_success:
					retry_count+=1
					if retry_count > 5:
						raise RuntimeError("Max retries exceeded while requesting maxmin timestamp on index '{}'".format(name))
					time.sleep(1)

			max_datetime = max_datetime_from_maxmin(maxmin_obj)
			min_datetime = min_datetime_from_maxmin(maxmin_obj)

			# Delete if found time range doesn't collide with selected time range
			if  min_datetime is not None \
				and max_datetime is not None \
				and datetime_range_a_is_within_b(
					min_datetime, max_datetime,
					args_min_datetime, args_max_datetime
				):
				print("Deleting index {}".format(name))
				result = req_delete_index(url, name)
				delete_count+=1
			else:
				print("Index {} won't be deleted.".format(name))
				if was_closed:
					print("Closing index {}".format(name))
					result = req_close_index(url, name)

		except KeyboardInterrupt:
			return

		except Exception as e:
			print("Error in this index:")
			pprint.pprint(index)

	print("Original count of indicies: {}".format(len(indices)))
	print("Count of deleted indicies: {}".format(delete_count))
	print("Count of reamining indicies: {}".format(len(indices)-delete_count))


def COMMAND_shrink(URL, index, backup_node=None, shrunk_prefix="shrunk_"):
	url = URL
	if url[-1:] == '/':
		url = url[:-1]

	# check index health, must be green
	r = requests.get("{}/_cat/indices?format=json".format(url))
	indices = r.json()

	if not backup_node:
		# pick a random warm node
		# TODO: how to tell if a node is warm?
		r = requests.get("{}/_cat/nodeattrs?format=json".format(url))
		warm_nodes = []
		for entry in r.json():
			if entry["attr"] == "data" and entry["value"] == "????????? WARM":
				warm_nodes.append(entry["node"])
		if not warm_nodes:
			raise RuntimeError("Warm node detection failed.")
		backup_node = random.choice(warm_nodes)

		# or pick a warm node with least indices
		# TODO: how to tell number of indices on a node?

	# prepare for shrink
	prepare_for_shrink = {
		"settings": {
			"index.number_of_replicas": 0,
			"index.routing.allocation.require._name": backup_node,
			"index.blocks.write": True
		}
	}
	r = requests.put("{}/{}/_settings".format(url, index), json=prepare_for_shrink)
	result = r.json()
	if not result.get("acknowledged"):
		print("Pre-shrink preparation failed for index '{}': {}".format(index, result))
		return

	# shrink
	r = requests.post("{url}/{index}/_shrink/{prefix}{index}".format(url=url, index=index, prefix=shrunk_prefix))
	result = r.json()
	if not result.get("acknowledged"):
		print("Shrink of index '{}' returned error: {}".format(index, result))


def COMMAND_forcemerge(URL, max_cpu_load, backup_node, exclude_prefix):
	url = URL
	if url[-1:] == '/':
		url = url[:-1]

	print("Communicating with: {}".format(url))

	print("Retrieving CPU stats...")
	url_get_os_stats = "{}/_nodes/stats/os/".format(url)
	r = requests.get(url_get_os_stats)
	stats = r.json()
	cpu_usage = {}
	nodes_memory = []
	for node_id, node_details in stats["nodes"].items():
		cpu_usage[node_details["name"]] = node_details["os"]["cpu"]["percent"]
		nodes_memory.append((
			"data" in node_details["roles"],
			node_details["os"]["mem"]["free_in_bytes"],
			node_details["name"]))
	nodes_memory.sort(reverse=True)

	# If no backup node specified, use the one with most free memory
	if not backup_node:
		backup_node = nodes_memory[0][2]

	# for k, v in cpu_usage.items():
	# 	print(k, v)
	# for v in nodes_memory:
	# 	print(v)
	get_indices = "{}/_cat/indices?format=json".format(url)
	r = requests.get(get_indices)
	indices = r.json()

	not_green_count = 0

	indices = sorted(indices, key=lambda index: index['index'].rsplit('_')[-1], reverse=False)
	failed = 0
	successful = 0
	for index in indices:
		try:
			name = index['index']

			if name.startswith('.'):
				print("Index {} will be excluded. Indexes that start with '.' are always excluded".format(name))
				continue

			if exclude_prefix is not None and name.startswith(exclude_prefix):
				print("Index {0} will be excluded because it matches prefix {1}".format(name, exclude_prefix))
				continue

			if index['health'] != 'green':
				print("Index {index} health is {health} - skipping".format(**index))
				not_green_count += 1
				continue

			if index['status'] != 'open':
				continue

			prepare_for_shrink = {
				"settings": {
					"index.number_of_replicas": 0,
					"index.routing.allocation.require._name": backup_node,
					"index.blocks.write": True
				}
			}

			r = requests.put("{}/{}/_settings".format(url, index["name"]), json=prepare_for_shrink)
			result = r.json()
			if not result.get('acknowledged'):
				print("Pre-shrink preparation failed for index '{}': {}".format(name, result))
				failed += 1
				continue

			requests.post("{}/{}/_forcemerge?&max_num_segments=1".format(url, index["name"]))
			result = r.json()
			if not result.get('acknowledged'):
				print("Force merge of index '{}' returned error: {}".format(name, result))
				failed += 1
				continue

			r = requests.post("{url}/{index}/_shrink/shrunk_{index}".format(url=url, index=index["name"]))
			result = r.json()
			if not result.get('acknowledged'):
				print("Shrink of index '{}' returned error: {}".format(name, result))
				failed += 1
				continue
			successful += 1

		except KeyboardInterrupt:
			return
		except Exception as e:
			print("Error in index '{}': {}".format(index["name"], e))
			# pprint.pprint(index)

	print("Count of total indicies: {}".format(len(indices)))
	print("Count of non-green indicies: {}".format(not_green_count))
	print("Failed shrinks: {}".format(failed))
	print("Successfully shrunk: {}".format(successful))


def main():
	# Get arguments
	args = parse_cmdline()

	# Call the command
	if 'COMMAND' not in args:
		print("Please select a command: load_index_template, cleanup, reopen, close.")
		print("For more information see --help")
		return 1

	if args.COMMAND == 'load_index_template':
		return COMMAND_load_index_template(args.DIR, args.URL)

	elif args.COMMAND == 'cleanup':
		return COMMAND_cleanup(args.URL, args.min_date, args.max_date, args.exclude)

	elif args.COMMAND == 'reopen':
		return COMMAND_reopen(args.URL, args.min_date, args.max_date, args.exclude)

	elif args.COMMAND == 'close':
		return COMMAND_close(args.URL, args.min_date, args.max_date, args.exclude)

	elif args.COMMAND == 'delete':
		return COMMAND_delete(args.URL, args.min_date, args.max_date, args.exclude)

	elif args.COMMAND == 'force-merge':
		return COMMAND_forcemerge(args.URL, args.max_cpu_load, args.backup_node, args.exclude)

	elif args.COMMAND == 'shrink':
		return COMMAND_shrink(args.URL, args.index, args.backup_node, args.shrunk_prefix)


if __name__ == '__main__':
	main()
