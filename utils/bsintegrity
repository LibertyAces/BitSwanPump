import argparse
import os
import json
import pprint
import re
import datetime
import time
import requests
import jwt


class Integrity:

	def __init__(self):
		self.Counter = 0
		self.HitCount = 0
		self.MissCount = 0
		# LoopCount helps to preserve while loop not break after 1st iteration 
		self.LoopCount = 0
		self.HashSet = {}


	# Return count of all items for given index
	def get_items_count_in_index(self, es_url, index_name):
		url = es_url + '/{}/_count'.format(index_name)
		r = requests.get(url, 
			json={'query':{'bool':{'must':{'match_all':{}}}}}, 
			headers={'Content-Type': 'application/json'})
		if r.status_code != 200:
			print("Failed to fetch data from ElasticSearch: {} from {}\n{}".format(r.status_code, url, r.text))
			return None
		else:
			msg = r.json()
			return msg['count']


	# Open a file with key
	def open_keyfile(self, key_path):
		# Check if the key path is set
		private_key = None
		if key_path == '' or key_path is None:
			private_key = None
		else:
			with open(key_path, 'r') as file:
				private_key = file.read()
		return private_key


	# Check for hash
	def object_check(self, JSONobject, private_key, algorithm):
		if 'hash' in JSONobject:
			decode = jwt.decode(JSONobject["hash"], private_key, algorithm=algorithm)
			self.compare(JSONobject, decode)
		else:
			for key in JSONobject:
				if type(JSONobject[key]) is dict:
					self.object_check(JSONobject[key], private_key, algorithm)


	# Hash objects
	def object_hash(self, JSONobject, private_key, algorithm):
		# Check if previous hash present in JSON and if so, delete it from JSON and store it as previous hash
		previous_hash = JSONobject.pop("hash", None)
		# Hash event
		JSONobject["hash"] = jwt.encode(JSONobject, self.JWTPrivateKey, algorithm=self.Algorithm).decode("utf-8")
		# Add recent and previous hash to hash set
		self.Counter += 1
		self.HashSet.update({str(self.Counter): {"hash": JSONobject["hash"], "prev_hash": previous_hash}})
		# TODO: Sending the hashed data to ES


	# Compare data
	def compare(self, original_data, decoded_data):
		# Removing hash from the decoded data if there is any
		previous_hash = decoded_data.pop("hash", None)
		# And appending it with recent hash to the HashSet dictionary
		if 'hash' in original_data:
			self.Counter += 1
			self.HashSet.update({str(self.Counter): {"hash": original_data["hash"], "prev_hash": previous_hash}})

		for key in original_data:
			# Avoiding comparison on hash key to prevent incomparability
			if str(key) != 'hash':
				# Recursive iteration through nested dictionaries
				if type(original_data[key]) is dict:
					self.compare(original_data[key], decoded_data[key])
				else:
					# Counting successful and unsuccessful hits on items
					if original_data[key] == decoded_data[key]:
						self.HitCount += 1
					else:
						self.MissCount += 1

	# Progress bar
	def progress_bar(self, iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', printEnd="\r"):
	    percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
	    filledLength = int(length * iteration // total)
	    bar = fill * filledLength + '-' * (length - filledLength)
	    print('\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)
	    # Print New Line on Complete
	    if iteration == total: 
	        print()


	def COMMAND_hash(self, URL_source, URL_sink, index, key_path, algorithm, items_size, scroll_time):
		count = self.get_items_count_in_index(URL_source, index)
		private_key = self.open_keyfile(key_path)
		url = URL
		if algorithm == '' or algorithm is None:
			algorithm = 'HS256'
		if count is None:
			return
		if count > 0:
			self.LoopCount = count
			self.progress_bar(0, count, prefix='Loading and hashing progress:', 
				suffix='Loading and hashing complete', length=50)
			while True:
				if not self.LoopCount <= 0:
					r = requests.post(url, 
						json={"size":items_size,'query': {'bool': {'must': {'match_all': {}}}}}, 
						headers={'Content-Type': 'application/json'})
					
					if r.status_code != 200:
						print("Failed to fetch data from ElasticSearch: {} from {}\n{}".format(r.status_code, url, r.text))
						break

					msg = r.json()
					hits = msg['hits']['hits']
					if len(hits) == 0:
						break

					# Encoding by object
					if private_key is None:
						break
					else:
						for JSONobject in hits:
							self.object_hash(JSONobject, private_key, algorithm)
					self.LoopCount -= int(items_size)

					time.sleep(0.1)
			    	# Update Progress Bar
					self.progress_bar(self.Counter, count, prefix='Progress:', suffix='Complete', length=50)
				else:
					break

		print(self.HashSet)


	def COMMAND_check(self, URL, index, key_path, algorithm, items_size, scroll_time):
		count = self.get_items_count_in_index(URL, index)
		private_key = self.open_keyfile(key_path)
		url = URL + '/{}/_search?scroll={}m'.format(index, scroll_time)
		if algorithm == '' or algorithm is None:
			algorithm = 'HS256'
		if count is None:
			return
		if count > 0:
			self.LoopCount = count
			self.progress_bar(0, count, prefix='Progress:', suffix='Complete', length=50)
			while True:
				if not self.LoopCount <= 0:
					r = requests.post(url, 
						json={"size":items_size,'query': {'bool': {'must': {'match_all': {}}}}}, 
						headers={'Content-Type': 'application/json'})
					
					if r.status_code != 200:
						print("Failed to fetch data from ElasticSearch: {} from {}\n{}".format(r.status_code, url, r.text))
						break

					msg = r.json()
					hits = msg['hits']['hits']
					if len(hits) == 0:
						break

					# Decoding hashes by object
					if private_key is None:
						break
					else:
						for JSONobject in hits:
							self.object_check(JSONobject, private_key, algorithm)
					self.LoopCount -= int(items_size)

					time.sleep(0.1)
			    	# Update Progress Bar
					self.progress_bar(self.Counter, count, prefix='Progress:', suffix='Complete', length=50)
				else:
					break

		print(self.HashSet)
		print('\n')
		print('Integrity has been approved on {} items\n'.format(str(self.HitCount)))
		print('Integrity has not been enthroned on {} items\n'.format(str(self.MissCount)))
		  




def parse_cmdline():

	# Parse args
	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawDescriptionHelpFormatter,
		description='''Hash the events from ElasticSearch.\n
		Check integrity on ElasticSearch hashes.\n

		-----------------------------------------------------

		Supported algorithms for cryptographic signing, default is HS256


		HS256 - HMAC using SHA-256 hash algorithm (default)
		HS384 - HMAC using SHA-384 hash algorithm
		HS512 - HMAC using SHA-512 hash algorithm
		ES256 - ECDSA signature algorithm using SHA-256 hash algorithm
		ES384 - ECDSA signature algorithm using SHA-384 hash algorithm
		ES512 - ECDSA signature algorithm using SHA-512 hash algorithm
		RS256 - RSASSA-PKCS1-v1_5 signature algorithm using SHA-256 hash algorithm
		RS384 - RSASSA-PKCS1-v1_5 signature algorithm using SHA-384 hash algorithm
		RS512 - RSASSA-PKCS1-v1_5 signature algorithm using SHA-512 hash algorithm
		PS256 - RSASSA-PSS signature using SHA-256 and MGF1 padding with SHA-256
		PS384 - RSASSA-PSS signature using SHA-384 and MGF1 padding with SHA-384
		PS512 - RSASSA-PSS signature using SHA-512 and MGF1 padding with SHA-512''')

	subparsers = parser.add_subparsers(help='commands')


	# An hash command
	hash_parser = subparsers.add_parser('hash', help='hash the events')
	hash_parser.add_argument('URL_source', action='store', help='a ElasticSearch URL source')
	hash_parser.add_argument('URL_sink', action='store', help='a ElasticSearch URL sink')
	hash_parser.add_argument('index', action='store', help='a ElasticSearch index')
	hash_parser.add_argument('key_path', action='store', help='a Path to a file with key')
	hash_parser.add_argument('algorithm', action='store', help='a Algorithms to use for decoding. More info in Description')
	hash_parser.add_argument('items_size', action='store', help='a Number of the items loaded from ES')
	hash_parser.add_argument('scroll_time', action='store', help='a ES Scroll time')
	hash_parser.set_defaults(COMMAND='hash')

	# An check command
	check_parser = subparsers.add_parser('check', help='check on integrity of hashed events')
	check_parser.add_argument('URL', action='store', help='a ElasticSearch URL')
	check_parser.add_argument('index', action='store', help='a ElasticSearch index')
	check_parser.add_argument('key_path', action='store', help='a Path to a file with key')
	check_parser.add_argument('algorithm', action='store', help='a Algorithms to use for decoding. More info in Description')
	check_parser.add_argument('items_size', action='store', help='a Number of the items loaded from ES')
	check_parser.add_argument('scroll_time', action='store', help='a ES Scroll time')
	check_parser.set_defaults(COMMAND='check')

	return parser.parse_args()



def main():
	# Get arguments
	args = parse_cmdline()

	# Call the command
	if 'COMMAND' not in args:
		print("Please select a command: hash, check.")
		print("For more information see --help")
		return 1

	if args.COMMAND == 'hash':
		return Integrity().COMMAND_hash(args.URL, args.key_path, args.algorithm)

	elif args.COMMAND == 'check':
		return Integrity().COMMAND_check(args.URL, args.index, args.key_path, args.algorithm, 
			args.items_size, args.scroll_time)


if __name__ == '__main__':
	main()
